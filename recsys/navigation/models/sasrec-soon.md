# SASREC \[SOON]

### TODO LIST:

* [ ] Про Дропаут
* [ ] Про влияние масштабирования модельки от юзеров (не влияет рексис всегда маленькие сеты)

***

## Интуиция

Когда мы говорим о рекомендательных системах, одна из важнейших задач — это предсказать, какие товары могут заинтересовать пользователя в будущем на основе его предыдущих взаимодействий. Представьте себе, что вы смотрите любимый сериал.

Платформа старается предсказать, что вам захочется посмотреть следующим. Это требует анализа последовательности ваших просмотров, ведь недавние действия могут быть гораздо более значимыми для текущих рекомендаций.

\[**Иллюстрация последовательности товаров**]

Вот здесь на сцену выходит SASRec — Self-Attentive Sequential Recommendation. В отличие от традиционных методов, которые могут просто учитывать, что вам нравится определенный жанр или актёр, SASRec смотрит на последовательность взаимодействий и пытается уловить закономерности в этой последовательности.

Почему это важно? Давайте представим, что вы недавно посмотрели несколько эпизодов научно-фантастического сериала. Скорее всего, ваша следующая заинтересованность будет также связана с научной фантастикой, а не с комедией, которую вы смотрели месяц назад. SASRec использует эту интуицию и пытается выявить, какие товары или контент вероятнее всего будут интересны пользователю в ближайшее время.

Основная идея SASRec основана на модели self-attention, которая изначально была предложена для задач в области обработки естественного языка, таких как машинный перевод. Self-attention позволяет модели сосредоточиться на наиболее значимых частях последовательности, игнорируя менее важные. Применяя эту концепцию к рекомендательным системам, SASRec рассматривает не просто список взаимодействий пользователя, но акцентирует внимание на тех элементах, которые имеют наибольшее значение для текущего момента времени.

**\[Графическое представление механизма self-attention, показывающее взаимодействие между различными элементами последовательности товаров.]**

Эта способность выделять важные элементы из последовательности делает SASRec чрезвычайно мощной для создания персонализированных рекомендаций. Она не только смотрит на последнее взаимодействие пользователя, но и учитывает все предыдущие, понимая, какие из них наиболее релевантны в данный момент. Таким образом, SASRec помогает рекомендательным системам стать умнее и более адаптивными, что в конечном итоге улучшает пользовательский опыт.

***

## Архитектура SASRec

В основе SASRec лежит идея использования self-attention механизма для моделирования последовательностей пользовательских действий. Это позволяет модели учитывать как ближайшие, так и более отдаленные взаимодействия, при этом не теряя важную информацию о порядке следования товаров.

**\[картинка общей архитектуры]**

## Embedding Layer

Начнем с самого начала. Когда пользователь взаимодействует с различными товарами, эти взаимодействия записываются как последовательность действий. Но для того чтобы модель могла эффективно работать с этими данными, нам нужно преобразовать их в удобный для обработки формат.

Embedding Layer выполняет эту задачу. Представь, что у нас есть список товаров, которые пользователь просматривал или покупал. Мы преобразуем этот список в последовательность фиксированной длины. Если в списке слишком много товаров, мы берем только последние несколько. Если товаров меньше, чем требуется, мы добавляем специальные «пустые» элементы, чтобы длина последовательности соответствовала нужному формату.

Теперь каждый товар в этом списке преобразуется в вектор — набор чисел, который описывает этот товар. Эти векторы позволяют модели работать с товарами как с числовыми данными, что очень важно для машинного обучения. Вектор для каждого товара содержит информацию о его характеристиках, которая помогает модели понимать, как товары связаны между собой и с действиями пользователя.

Этот процесс преобразования списка товаров в векторные представления и есть работа Embedding Layer. Он готовит данные для дальнейшей обработки моделью, позволяя ей захватывать и анализировать последовательности действий пользователя.

Теперь, когда мы разобрались с Embedding Layer, давай перейдем к следующему компоненту — Self-Attention Layer.

## **Positional Embedding**

Когда мы говорим о последовательных данных, важно учитывать не только сами товары, но и их порядок. Вспомни, когда ты читаешь книгу: каждое слово важно не само по себе, а в контексте предыдущих и последующих слов. В рекомендательных системах порядок взаимодействий с товарами также играет большую роль.

Однако self-attention механизм, используемый в SASRec, сам по себе не знает, в каком порядке идут товары. Чтобы решить эту проблему, мы используем **Positional Embedding**. Это похоже на добавление меток времени к каждому товару в последовательности, чтобы модель знала, какой товар был просмотрен или куплен раньше, а какой позже.

Мы добавляем к каждому вектору товара дополнительный вектор, который содержит информацию о позиции этого товара в последовательности. Эти векторы называются positional embeddings, и они обучаемы, то есть модель сама учится оптимальным образом представлять эту позиционную информацию в процессе обучения.

Вот как это работает: представь, что у нас есть векторное представление товара. Мы добавляем к нему позиционный вектор, соответствующий его позиции в последовательности. Этот объединенный вектор затем используется на следующих этапах обработки данных в модели.

Пробовали также использовать фиксированные позиционные векторы, но оказалось, что обучаемые векторы дают лучшие результаты. Это объясняется тем, что обучаемые векторы могут лучше адаптироваться к особенностям данных и задач, с которыми работает модель.

Таким образом, Positional Embedding позволяет модели учитывать порядок взаимодействий пользователей с товарами, что является ключевым для понимания последовательностей и улучшения рекомендаций.

**TODO: дописать**

## Self-Attention Layer

Теперь мы подходим к одному из самых важных компонентов SASRec — Self-Attention Layer. Этот механизм позволяет модели сосредоточиться на различных частях последовательности, чтобы понять, какие элементы наиболее важны для предсказания следующего действия.

Представь, что ты читаешь длинный список товаров, которые покупал или просматривал пользователь. Чтобы предсказать, что пользователь захочет посмотреть или купить дальше, нам нужно понять, какие из предыдущих товаров имеют наибольшее влияние на текущее состояние. Это как если бы ты пытался вспомнить, какие книги ты читал до того, чтобы решить, что читать дальше

Self-attention делает именно это. Он позволяет модели вычислять взвешенные суммы всех товаров в последовательности, где вес каждого товара указывает на его важность для текущего предсказания. Другими словами, self-attention позволяет модели обратить внимание на наиболее релевантные товары в последовательности.

### Как работает Self-Attention

Self-attention вычисляет внимание (attention) для каждого элемента последовательности по отношению ко всем остальным элементам. Это позволяет модели учитывать взаимодействия между любыми двумя элементами последовательности, независимо от их расстояния друг от друга.

Процесс self-attention начинается с трех основных матриц: Queries (запросы), Keys (ключи) и Values (значения). Эти матрицы получают из входных векторных представлений с помощью линейных преобразований. Давайте подробнее рассмотрим, как это происходит.

**\[картинка self attention]**

### Линейные Преобразования

**TODO: раписать поподробнее и интуитивно понятнее с картинками**

Для каждого элемента входной последовательности $\hat{E}$ (которая включает positional embedding), мы создаем три матрицы:

* **Query Matrix (Q)**: $Q=\hat{E} W^Q$
* **Key Matrix (K)**: $K=\hat{E} W^K$
* **Value Matrix (V)**: $V=\hat{E} W^V$

Здесь $W^Q$, $W^K$, $W^V$ — это **обучаемые веса**, которые преобразуют входные векторы в соответствующие матрицы.

После этого мы можем вычислить внимание (attention) для каждого элемента последовательности с помощью формулы:

$$
\text{Attention(Q,K,V)} = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})
$$

## Causality

При работе с последовательностями важно учитывать, что модель не должна использовать информацию из будущего для предсказания текущего или прошлого элемента. Это особенно важно для последовательных рекомендательных систем, где необходимо предсказывать следующий элемент на основе предыдущих.

### Проблема с Self-Attention

В обычном механизме self-attention каждый элемент последовательности может учитывать информацию от всех остальных элементов, включая будущие. Однако при предсказании следующего элемента последовательности это приводит к некорректной постановке задачи, потому что модель может использовать информацию из будущих элементов, которые она ещё не должна знать.

Чтобы устранить эту проблему, SASRec вводит ограничение на механизм внимания. Это ограничение запрещает каждому элементу последовательности использовать информацию из будущих элементов. Технически это достигается путем маскировки (masking) в self-attention механизме.

### Маскировка в Self-Attention

При вычислении внимания для каждого элемента $Q\_i$, SASRec запрещает все ссылки на элементы $K\_j$, где $j>i$.

Это означает, что каждый элемент $Q\_i$ может учитывать только те элементы $K\_j$, которые предшествуют ему или находятся на той же позиции.

В реализации это достигается с помощью маскирующей матрицы, которая применяется перед softmax функцией в self-attention механизме. Эта маска блокирует нежелательные связи, обеспечивая причинность.

**\[IMAGE: MASKED SELF ATTENTION]**

## Point-Wise Feed-Forward Network (FFN)

Несмотря на то, что self-attention позволяет модели учитывать все предыдущие элементы последовательности с адаптивными весами, это все же линейная модель. Чтобы добавить нелинейность и учесть взаимодействия между различными скрытыми измерениями, SASRec использует point-wise feed-forward сеть. Давай разберем, как это работает.

### Зачем нужна Feed-Forward Network

Self-attention хорош для определения важности различных элементов последовательности, но его линейная природа ограничивает способность модели захватывать сложные паттерны. Для преодоления этой проблемы SASRec использует двухслойную feed-forward нейронную сеть после каждого self-attention слоя. Это позволяет модели обучаться более сложным зависимостям в данных.

### Как работает Feed-Forward Network

Для каждого элемента $S\_i$, который является выходом self-attention слоя, применяется двухслойная feed-forward сеть. Обозначим это преобразование как $F\_i = \text{FFN}(S\_i)$

FFN состоит из двух линейных преобразований, разделенных активационной функцией ReLU (Rectified Linear Unit):

$$
F_i = \text{ReLU}(S_i W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}
$$

Где:

* $S\_i$ — это выход self-attention слоя для элемента $i$
* $W^{(1)}$ и $W^{(2)}$ — обучаемые матрицы весов размерности $d \times d$
* $b^{(1)}$ и $b^{(2)}$ — обучаемые смещения (bias) размерности $d$

## Stacking Self-Attention Blocks

В модели SASRec мы можем улучшить её способность захватывать сложные переходы между элементами, используя несколько блоков self-attention. Эта техника называется "stacking", то есть наложение нескольких слоёв self-attention и feed-forward сетей.

### Первый Блок

Первый блок self-attention в модели определяет начальные зависимости между элементами последовательности. Этот блок включает в себя как self-attention слой, так и point-wise feed-forward сеть. Обозначим результаты первого блока как $S^{(1)}$ и $F^{(1)}$,

## Residual Connections

## Layer normalization

## Dropout

## Shared Item Embedding

## Explicit User Modeling

***

## Обучение

***

## Инференс

```python
# Пайплайн обучения

# пример взаимодействий пользователя с итемами
- 101, 205, 316, 427, 538 

# представим что размер входного эмбеддинга у нас 7
# добавим паддинг в начало последовательности
- 0, 0, 101, 205, 316, 427, 538 

**# преобразуем вектор в эмбеддинг**								
**item_emb = self.item_embedding(item_seq) 

# добавим обучаемые позиционные эмбеддинги 
# в классическом трансформере используются фиксированные синусоидные 
# для четных PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
# для нечетных PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)
position_ids = position_ids.unsqueeze(0).expand_as(item_seq)
position_embedding = self.position_embedding(position_ids)
input_emb = item_emb + position_embedding

# используется нормализация
input_emb = self.LayerNorm(input_emb)

# применяется дропаут к входному эмбеддингу 
input_emb = self.dropout(input_emb)

# определяем, на какие элементы последовательности может "смотреть" при
# вычислении внимания (в данной реализации маска исключает 0 - т.к. это паддинг).**
attention_mask = self.get_attention_mask(item_seq)

# тут мы заходим в энкодер блок
class TransformerLayer(nn.Module):
    def forward(self, hidden_states, attention_mask):
			  # multihead attention
        attention_output = self.multi_head_attention(hidden_states, attention_mask)
        # dropout
        attention_output = self.attention_dropout(attention_output)
        # add & norm
        attention_output = self.attention_layer_norm(hidden_states + attention_output)
        # Двухслойная feed-forward сеть с активацией между слоями.
        intermediate_output = self.intermediate_dense(attention_output)
        intermediate_output = self.intermediate_act_fn(intermediate_output)
        layer_output = self.output_dense(intermediate_output)
        # dropout
        layer_output = self.output_dropout(layer_output)
        # add & norm
        layer_output = self.output_layer_norm(attention_output + layer_output)
        
        return layer_output

# выбирается выход последнего блока энкодера
trm_output = self.trm_encoder(input_emb, attention_mask, output_all_encoded_layers=True)
output = trm_output[-1]

# получаем финальное представление для каждой последовательности
output = self.gather_indexes(output, item_seq_len - 1)

# преобразуем полученное представление в распределение вероятностей по всем возможным элементам
logits = self.prediction_layer(output)

# применяем софтмакс для получения вероятностей
probabilities = torch.softmax(logits, dim=-1)

# выбираем топ-k элементов с наибольшей вероятностью
_, top_items = torch.topk(probabilities, k=10)

# определяем функцию потерь (в данном случае, кросс-энтропия)
loss = nn.CrossEntropyLoss()(logits, target_items)

# выполняем обратное распространение ошибки
loss.backward()

# обновляем параметры модели
optimizer.step()

# обнуляем градиенты
optimizer.zero_grad()

# вычисляем метрики
with torch.no_grad():
    # Hit Rate@10
    hit_rate = torch.any(top_items == target_items.unsqueeze(-1), dim=-1).float().mean()
    
    # NDCG@10
    rank = torch.where(top_items == target_items.unsqueeze(-1))[1] + 1
    ndcg = (1 / torch.log2(rank.float() + 1)).mean()
```
