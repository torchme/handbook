# MAP

Mean Average Precision — одна из метрик, которая помогает оценить качество систем ранжирования и рекомендаций. Она измеряет как релевантность предлагаемых элементов, так и то, насколько хорошо система размещает более релевантные элементы вверху.

$$
\text{MAP@K} = \frac{1}{U} \sum_{u=1} ^{U} \text{AP@K}_{u}
$$

**где:**

U — общее число пользователей в датасете

AP — это средний Precision для ранжирующего списка

## Precision

**Precision**

## Average Precision @K

**Average Precision @K** вычисляется как среднее значение **Precision** во всех relevant позициях внутри K. Мы можем выразить это следующим образом:

$$
\text{AP@K} = \frac{1}{N} \sum_{k=1}^{k} \text{Precision(k)} \times \text{rel(k)}
$$

**где:**

**N** — полное число релевантных объектов для конкретного пользователя

**$\text{Precision@k}$** — точность для каждой позиции

**$\text{rel@k}$** — Если объект релевантный: 1, если не релевантный 0. (Например купил товар, кликнул на карточку, посмотрел, …)

Допустим, у нас есть шесть рекомендованных элементов, из которых три являются релевантными. Обычный Precision@6 в этом случае будет 50%. Однако значение средней точности (Average Precision) будет варьироваться в зависимости от порядка ранжирования.

Предположим, что релевантные элементы находятся на позициях 1, 4 и 5. В этом случае средняя точность будет 70%. Позиции 2, 3 и 6 содержат нерелевантные элементы и не влияют на расчёт AP.

Что, если все релевантные рекомендации окажутся в верхней части списка? В этом случае мы получим идеальное ранжирование, и средняя точность будет равна 1.

Давайте придумаем еще несколько комбинаций, чтобы проиллюстрировать поведение метрики. Мы сохраним общее количество релевантных элементов тем же: 3 из 6, но рассмотрим разные порядки ранжирования.

## MAP@K

Значения MAP могут варьироваться от 0 до 1. Чем выше значение MAP, тем лучше система размещает релевантные элементы высоко в списке.

* MAP равно 1 в случае идеального ранжирования, когда все релевантные документы или элементы находятся в верхней части списка.
* MAP равно 0, когда не найдено ни одного релевантного объекта.
* Во всех остальных случаях MAP может быть между 0 и 1. Чем ближе значение MAP к 1, тем лучше качество ранжирования.

Метрика MAP поощряет способность системы размещать релевантные элементы в верхней части списка.

Предположим, вы смотрите на топ-10 результатов поиска. В идеальном сценарии все документы на странице должны быть релевантными. Но что, если релевантными окажутся только пара из них? В этом случае они должны появляться в верхней части страницы, а не в нижней. Именно такое поведение поощряет метрика MAP.

Это интуитивно соответствует "хорошему" поведению системы, например, поиска. Однако небольшие изменения в значении MAP между 0 и 1 могут быть менее интуитивными. В отличие от простых Precision или Recall, MAP не имеет непосредственной реальной интерпретации.

### Интуиция

По сути, средняя точность (Average Precision) предоставляет одно значение, агрегирующее точность модели на различных уровнях Recall. Напомним, что Recall отражает долю правильно найденных релевантных элементов из общего числа релевантных элементов в наборе данных.

**Давайте рассмотрим этот расчет:**

* Вы перемещаетесь по ранжированному списку, ища релевантные элементы.
* Каждый раз, когда вы находите новый релевантный элемент, ваш Recall увеличивается (поскольку вы добавляете еще один правильно найденный положительный результат в числитель формулы Recall). Бинго! На каждом таком ранге вы вычисляете соответствующую точность.
* Если вы не находите релевантный элемент, Recall остается прежним. Это элементы, которые вы не рассматриваете. Вы пропускаете вычисление точности, поскольку Recall не изменяется.
* Затем вы агрегируете точность, вычисленную во всех позициях, где элемент был релевантен.

Звучит знакомо? Мы делали то же самое, когда вводили формулу **AP** в начале статьи. Мы просто говорили о вычислении точности на "каждом релевантном ранге", вместо "когда значение Recall изменяется". Но в конечном итоге это одно и то же. Это предлагает свежий взгляд на AP, рассматривая его как форму взвешенной точности. Вы рассматриваете точность в точках увеличения Recall и игнорируете ее в противном случае.

Когда мы говорим о вычислении точности на "каждом релевантном ранге", мы имеем в виду, что для каждой позиции в списке рекомендаций, где находится релевантный (то есть "положительный") элемент, мы вычисляем точность (Precision). Это означает, что мы рассматриваем список рекомендаций и при каждом обнаружении релевантного элемента пересчитываем точность исходя из всех предыдущих рекомендаций до этого места.

Вы можете визуализировать это, используя кривую Precision-Recall. Она отображает значения точности против различных значений Recall на каждом K. Это помогает визуализировать ступенчатые изменения, когда вы перемещаетесь по ранжированному списку. Затем вы можете думать о средней точности как об интерполированной области под кривой Precision-Recall.

Например, давайте отобразим значения для сценария с 6 рангами и 3 всего релевантными элементами. Из 3 релевантных элементов все три находятся в верхней части. Эта ситуация представляет идеальное ранжирование. Наша точность равна 1 на каждой релевантной позиции.

Поскольку все релевантные элементы находятся в верхней части, кривая остаётся плоской в верхней части графика до тех пор, пока вы не найдёте все три релевантных элемента и не достигнете максимального Recall. Довольно интуитивно понятно, что средняя точность (площадь результирующего квадрата) также равна одному.

Как это будет выглядеть в нашем втором сценарии?

В этом случае мы видим резкое падение после второго элемента — до тех пор, пока мы не встретим последний релевантный элемент в нижней части списка. Вы можете заметить типичный зигзагообразный узор, где точность немного подскакивает после того, как мы находим следующий релевантный элемент: так часто выглядит кривая precision-recall на практике. Тем не менее в этом случае общая площадь площади остаётся довольно большой (AP = 0.83): мы правильно определили первые два элемента, и их вклад значителен.

Мы начали довольно низко, поскольку первые предсказания не совпали. На первой релевантной позиции (ранг 4) наша точность составила всего 0.25. Хотя точность восстанавливается по мере продвижения, общая площадь под кривой остаётся меньше, что заметно по значению AP, равному 0.38.

Важно отметить, что этот пример показывает AP для полного ранжированного списка, а не AP при K. На практике у вас могут быть релевантные элементы за пределами K. Следовательно, вы не достигнете полноты (Recall) равной 1. Однако такая иллюстрация дает интуицию о том, как метрика штрафует за ранние ошибки. AP при K является частичным сводом кривой precision-recall. Хотя вы можете не рассчитывать AP при K как площадь под кривой, вы можете мыслить его концептуально как среднюю высоту кривой точности до ранга K.
